{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping Lab\n",
    "\n",
    "In this lab you will first learn the following code snippet which is a simple web spider class that allows you to scrape paginated webpages. Read the code, run it, and make sure you understand how it works. In the challenges of this lab, we will guide you in building up this class so that eventually you will have a more robust web spider that you can further work on in the Web Scraping Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class IronhackSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scrape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "\"\"\"\n",
    "This is a custom parser function you will complete in the challenge.\n",
    "Right now it simply returns the string passed to it. But in this lab\n",
    "you will complete this function so that it extracts the quotes.\n",
    "This function will be passed to the IronhackSpider class.\n",
    "\"\"\"\n",
    "def quotes_parser(content):\n",
    "    return content\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1 - Custom Parser Function\n",
    "\n",
    "In this challenge, complete the custom `quotes_parser()` function so that the returned result contains the quote string instead of the whole html page content.\n",
    "\n",
    "In the cell below, write your updated `quotes_parser()` function and kickstart the spider. Make sure the results being printed contain a list of quote strings extracted from the html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('span', {'class':'text'})\n",
    "    quotes2 = [quote.text.strip() for quote in quotes]\n",
    "\n",
    "    return quotes2\n",
    "\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 - Error Handling\n",
    "\n",
    "In `IronhackSpider.scrape_url()`, catch any error that might occur when you make requests to scrape the webpage. This includes checking the response status code and catching http request errors such as timeout, SSL, and too many redirects.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout occured')\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print('Too many redirects')\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSL error occured')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request exception')\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('span', {'class':'text'})\n",
    "    quotes2 = [quote.text.strip() for quote in quotes]\n",
    "\n",
    "    return quotes2\n",
    "\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Sleep Interval\n",
    "\n",
    "In `IronhackSpider.kickstart()`, implement `sleep_interval`. You will check if `self.sleep_interval` is larger than 0. If so, tell the FOR loop to sleep the given amount of time before making the next request.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout occured')\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print('Too many redirects')\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSL error occured')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request exception')\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):        \n",
    "        import time\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 1 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('span', {'class':'text'})\n",
    "    quotes2 = [quote.text.strip() for quote in quotes]\n",
    "\n",
    "    return quotes2\n",
    "\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Test Batch Scraping\n",
    "\n",
    "Change the `PAGES_TO_SCRAPE` value from `1` to `10`. Try if your code still works as intended to scrape 10 webpages. If there are errors in your code, fix them.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=2, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout occured')\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print('Too many redirects')\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSL error occured')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request exception')\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        import time\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 10 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('span', {'class':'text'})\n",
    "    quotes2 = [quote.text.strip() for quote in quotes]\n",
    "\n",
    "    return quotes2\n",
    "\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Scrape a Different Website\n",
    "\n",
    "Update the parameters passed to the `IronhackSpider` constructor so that you coder can crawl [books.toscrape.com](http://books.toscrape.com/). You will need to use a different `URL_PATTERN` (figure out the new url pattern by yourself) and write another parser function to be passed to `IronhackSpider`. \n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n",
      "['In Her Wake', 'How Music Works', 'Foolproof Preserving: A Guide ...', 'Chase Me (Paris Nights ...', 'Black Dust', 'Birdsong: A Story in ...', \"America's Cradle of Quarterbacks: ...\", 'Aladdin and His Wonderful ...', 'Worlds Elsewhere: Journeys Around ...', 'Wall and Piece', 'The Four Agreements: A ...', 'The Five Love Languages: ...', 'The Elephant Tree', 'The Bear and the ...', \"Sophie's World\", 'Penny Maybe', 'Maude (1883-1993):She Grew Up ...', 'In a Dark, Dark ...', 'Behind Closed Doors', \"You can't bury them ...\"]\n",
      "['Slow States of Collapse: ...', 'Reasons to Stay Alive', 'Private Paris (Private #10)', '#HigherSelfie: Wake Up Your ...', 'Without Borders (Wanderlove #1)', 'When We Collided', 'We Love You, Charlie ...', 'Untitled Collection: Sabbath Poems ...', 'Unseen City: The Majesty ...', 'Unicorn Tracks', 'Unbound: How Eight Technologies ...', 'Tsubasa: WoRLD CHRoNiCLE 2 ...', 'Throwing Rocks at the ...', 'This One Summer', 'Thirst', 'The Torch Is Passed: ...', 'The Secret of Dreadwillow ...', 'The Pioneer Woman Cooks: ...', 'The Past Never Ends', 'The Natural History of ...']\n",
      "['The Nameless City (The ...', 'The Murder That Never ...', 'The Most Perfect Thing: ...', 'The Mindfulness and Acceptance ...', 'The Life-Changing Magic of ...', 'The Inefficiency Assassin: Time ...', 'The Gutsy Girl: Escapades ...', 'The Electric Pencil: Drawings ...', 'The Death of Humanity: ...', 'The Bulletproof Diet: Lose ...', 'The Art Forger', 'The Age of Genius: ...', \"The Activist's Tao Te ...\", 'Spark Joy: An Illustrated ...', 'Soul Reader', 'Security', 'Saga, Volume 6 (Saga ...', 'Saga, Volume 5 (Saga ...', 'Reskilling America: Learning to ...', 'Rat Queens, Vol. 3: ...']\n",
      "['Princess Jellyfish 2-in-1 Omnibus, ...', 'Princess Between Worlds (Wide-Awake ...', 'Pop Gun War, Volume ...', 'Political Suicide: Missteps, Peccadilloes, ...', 'Patience', 'Outcast, Vol. 1: A ...', 'orange: The Complete Collection ...', 'Online Marketing for Busy ...', 'On a Midnight Clear', 'Obsidian (Lux #1)', 'My Paris Kitchen: Recipes ...', 'Masks and Shadows', 'Mama Tried: Traditional Italian ...', 'Lumberjanes, Vol. 2: Friendship ...', 'Lumberjanes, Vol. 1: Beware ...', 'Lumberjanes Vol. 3: A ...', 'Layered: Baking, Building, and ...', 'Judo: Seven Steps to ...', 'Join', 'In the Country We ...']\n",
      "['Immunity: How Elie Metchnikoff ...', 'I Hate Fairyland, Vol. ...', 'I am a Hero ...', 'How to Be Miserable: ...', 'Her Backup Boyfriend (The ...', 'Giant Days, Vol. 2 ...', 'Forever and Forever: The ...', 'First and First (Five ...', 'Fifty Shades Darker (Fifty ...', 'Everydata: The Misinformation Hidden ...', \"Don't Be a Jerk: ...\", 'Danganronpa Volume 1', 'Crown of Midnight (Throne ...', 'Codename Baboushka, Volume 1: ...', 'Camp Midnight', 'Call the Nurse: True ...', 'Burning', 'Bossypants', 'Bitch Planet, Vol. 1: ...', 'Avatar: The Last Airbender: ...']\n",
      "['Algorithms to Live By: ...', 'A World of Flavor: ...', 'A Piece of Sky, ...', 'A Murder in Time', 'A Flight of Arrows ...', 'A Fierce and Subtle ...', 'A Court of Thorns ...', '(Un)Qualified: How God Uses ...', 'You Are What You ...', \"William Shakespeare's Star Wars: ...\", 'Tuesday Nights in 1980', 'Tracing Numbers on a ...', 'Throne of Glass (Throne ...', 'Thomas Jefferson and the ...', 'Thirteen Reasons Why', 'The White Cat and ...', 'The Wedding Dress', 'The Vacationers', 'The Third Wave: An ...', 'The Stranger']\n",
      "['The Shadow Hero (The ...', 'The Secret (The Secret ...', 'The Regional Office Is ...', 'The Psychopath Test: A ...', 'The Project', 'The Power of Now: ...', \"The Omnivore's Dilemma: A ...\", 'The Nerdy Nummies Cookbook: ...', 'The Murder of Roger ...', 'The Mistake (Off-Campus #2)', \"The Matchmaker's Playbook (Wingmen ...\", 'The Love and Lemons ...', 'The Long Shadow of ...', 'The Kite Runner', 'The House by the ...', 'The Glittering Court (The ...', 'The Girl on the ...', 'The Genius of Birds', 'The Emerald Mystery', 'The Cookies & Cups ...']\n",
      "['The Bridge to Consciousness: ...', \"The Artist's Way: A ...\", 'The Art of War', 'The Argonauts', 'The 10% Entrepreneur: Live ...', 'Suddenly in Love (Lake ...', 'Something More Than This', 'Soft Apocalypse', \"So You've Been Publicly ...\", 'Shoe Dog: A Memoir ...', 'Shobu Samurai, Project Aryoku ...', 'Secrets and Lace (Fatal ...', 'Scarlett Epstein Hates It ...', 'Romero and Juliet: A ...', 'Redeeming Love', 'Poses for Artists Volume ...', 'Poems That Make Grown ...', 'Nightingale, Sing', 'Night Sky with Exit ...', 'Mrs. Houdini']\n",
      "['Modern Romance', 'Miss Peregrine’s Home for ...', 'Louisa: The Extraordinary Life ...', 'Little Red', 'Library of Souls (Miss ...', 'Large Print Heart of ...', 'I Had a Nice ...', 'Hollow City (Miss Peregrine’s ...', 'Grumbles', 'Full Moon over Noah’s ...', 'Frostbite (Vampire Academy #2)', 'Follow You Home', 'First Steps for New ...', 'Finders Keepers (Bill Hodges ...', 'Fables, Vol. 1: Legends ...', 'Eureka Trivia 6.0', 'Drive: The Surprising Truth ...', 'Done Rubbed Out (Reightman ...', 'Doing It Over (Most ...', 'Deliciously Ella Every Day: ...']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout occured')\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print('Too many redirects')\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSL error occured')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request exception')\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        import time\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 10 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('article', {'class':'product_pod'})\n",
    "    quotes2 = [book.text.strip().split('\\n') for book in quotes]\n",
    "    quotes3 = [book[0] for book in quotes2]\n",
    "    quotes3    \n",
    "    return quotes3\n",
    "\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1 - Making Your Spider Unblockable\n",
    "\n",
    "Use techniques such as randomizing user agents and referers in your requests to reduce the likelihood that your spider is blocked by websites. [Here](http://blog.adnansiddiqi.me/5-strategies-to-write-unblock-able-web-scrapers-in-python/) is a great article to learn these techniques.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the ...', 'Tipping the Velvet', 'Soumission', 'Sharp Objects', 'Sapiens: A Brief History ...', 'The Requiem Red', 'The Dirty Little Secrets ...', 'The Coming Woman: A ...', 'The Boys in the ...', 'The Black Maria', 'Starving Hearts (Triangular Trade ...', \"Shakespeare's Sonnets\", 'Set Me Free', \"Scott Pilgrim's Precious Little ...\", 'Rip it Up and ...', 'Our Band Could Be ...', 'Olio', 'Mesaerion: The Best Science ...', 'Libertarianism for Beginners', \"It's Only the Himalayas\"]\n",
      "['In Her Wake', 'How Music Works', 'Foolproof Preserving: A Guide ...', 'Chase Me (Paris Nights ...', 'Black Dust', 'Birdsong: A Story in ...', \"America's Cradle of Quarterbacks: ...\", 'Aladdin and His Wonderful ...', 'Worlds Elsewhere: Journeys Around ...', 'Wall and Piece', 'The Four Agreements: A ...', 'The Five Love Languages: ...', 'The Elephant Tree', 'The Bear and the ...', \"Sophie's World\", 'Penny Maybe', 'Maude (1883-1993):She Grew Up ...', 'In a Dark, Dark ...', 'Behind Closed Doors', \"You can't bury them ...\"]\n",
      "['Slow States of Collapse: ...', 'Reasons to Stay Alive', 'Private Paris (Private #10)', '#HigherSelfie: Wake Up Your ...', 'Without Borders (Wanderlove #1)', 'When We Collided', 'We Love You, Charlie ...', 'Untitled Collection: Sabbath Poems ...', 'Unseen City: The Majesty ...', 'Unicorn Tracks', 'Unbound: How Eight Technologies ...', 'Tsubasa: WoRLD CHRoNiCLE 2 ...', 'Throwing Rocks at the ...', 'This One Summer', 'Thirst', 'The Torch Is Passed: ...', 'The Secret of Dreadwillow ...', 'The Pioneer Woman Cooks: ...', 'The Past Never Ends', 'The Natural History of ...']\n",
      "['The Nameless City (The ...', 'The Murder That Never ...', 'The Most Perfect Thing: ...', 'The Mindfulness and Acceptance ...', 'The Life-Changing Magic of ...', 'The Inefficiency Assassin: Time ...', 'The Gutsy Girl: Escapades ...', 'The Electric Pencil: Drawings ...', 'The Death of Humanity: ...', 'The Bulletproof Diet: Lose ...', 'The Art Forger', 'The Age of Genius: ...', \"The Activist's Tao Te ...\", 'Spark Joy: An Illustrated ...', 'Soul Reader', 'Security', 'Saga, Volume 6 (Saga ...', 'Saga, Volume 5 (Saga ...', 'Reskilling America: Learning to ...', 'Rat Queens, Vol. 3: ...']\n",
      "['Princess Jellyfish 2-in-1 Omnibus, ...', 'Princess Between Worlds (Wide-Awake ...', 'Pop Gun War, Volume ...', 'Political Suicide: Missteps, Peccadilloes, ...', 'Patience', 'Outcast, Vol. 1: A ...', 'orange: The Complete Collection ...', 'Online Marketing for Busy ...', 'On a Midnight Clear', 'Obsidian (Lux #1)', 'My Paris Kitchen: Recipes ...', 'Masks and Shadows', 'Mama Tried: Traditional Italian ...', 'Lumberjanes, Vol. 2: Friendship ...', 'Lumberjanes, Vol. 1: Beware ...', 'Lumberjanes Vol. 3: A ...', 'Layered: Baking, Building, and ...', 'Judo: Seven Steps to ...', 'Join', 'In the Country We ...']\n",
      "['Immunity: How Elie Metchnikoff ...', 'I Hate Fairyland, Vol. ...', 'I am a Hero ...', 'How to Be Miserable: ...', 'Her Backup Boyfriend (The ...', 'Giant Days, Vol. 2 ...', 'Forever and Forever: The ...', 'First and First (Five ...', 'Fifty Shades Darker (Fifty ...', 'Everydata: The Misinformation Hidden ...', \"Don't Be a Jerk: ...\", 'Danganronpa Volume 1', 'Crown of Midnight (Throne ...', 'Codename Baboushka, Volume 1: ...', 'Camp Midnight', 'Call the Nurse: True ...', 'Burning', 'Bossypants', 'Bitch Planet, Vol. 1: ...', 'Avatar: The Last Airbender: ...']\n",
      "['Algorithms to Live By: ...', 'A World of Flavor: ...', 'A Piece of Sky, ...', 'A Murder in Time', 'A Flight of Arrows ...', 'A Fierce and Subtle ...', 'A Court of Thorns ...', '(Un)Qualified: How God Uses ...', 'You Are What You ...', \"William Shakespeare's Star Wars: ...\", 'Tuesday Nights in 1980', 'Tracing Numbers on a ...', 'Throne of Glass (Throne ...', 'Thomas Jefferson and the ...', 'Thirteen Reasons Why', 'The White Cat and ...', 'The Wedding Dress', 'The Vacationers', 'The Third Wave: An ...', 'The Stranger']\n",
      "['The Shadow Hero (The ...', 'The Secret (The Secret ...', 'The Regional Office Is ...', 'The Psychopath Test: A ...', 'The Project', 'The Power of Now: ...', \"The Omnivore's Dilemma: A ...\", 'The Nerdy Nummies Cookbook: ...', 'The Murder of Roger ...', 'The Mistake (Off-Campus #2)', \"The Matchmaker's Playbook (Wingmen ...\", 'The Love and Lemons ...', 'The Long Shadow of ...', 'The Kite Runner', 'The House by the ...', 'The Glittering Court (The ...', 'The Girl on the ...', 'The Genius of Birds', 'The Emerald Mystery', 'The Cookies & Cups ...']\n",
      "['The Bridge to Consciousness: ...', \"The Artist's Way: A ...\", 'The Art of War', 'The Argonauts', 'The 10% Entrepreneur: Live ...', 'Suddenly in Love (Lake ...', 'Something More Than This', 'Soft Apocalypse', \"So You've Been Publicly ...\", 'Shoe Dog: A Memoir ...', 'Shobu Samurai, Project Aryoku ...', 'Secrets and Lace (Fatal ...', 'Scarlett Epstein Hates It ...', 'Romero and Juliet: A ...', 'Redeeming Love', 'Poses for Artists Volume ...', 'Poems That Make Grown ...', 'Nightingale, Sing', 'Night Sky with Exit ...', 'Mrs. Houdini']\n",
      "['Modern Romance', 'Miss Peregrine’s Home for ...', 'Louisa: The Extraordinary Life ...', 'Little Red', 'Library of Souls (Miss ...', 'Large Print Heart of ...', 'I Had a Nice ...', 'Hollow City (Miss Peregrine’s ...', 'Grumbles', 'Full Moon over Noah’s ...', 'Frostbite (Vampire Academy #2)', 'Follow You Home', 'First Steps for New ...', 'Finders Keepers (Bill Hodges ...', 'Fables, Vol. 1: Legends ...', 'Eureka Trivia 6.0', 'Drive: The Surprising Truth ...', 'Done Rubbed Out (Reightman ...', 'Doing It Over (Most ...', 'Deliciously Ella Every Day: ...']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "        \n",
    "\n",
    "#this function now includes a new function\n",
    "#it provides a random user agent from a list\n",
    "#the list extract the user agents from a txt file (user_agents.txt)\n",
    "#this file was obtaind from https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "#for every request we generate new headers with random user agent \n",
    "    def scrape_url(self, url):\n",
    "        \n",
    "        def get_random_ua():\n",
    "            import random\n",
    "            with open('user_agents.txt') as f:\n",
    "                lines = f.readlines()\n",
    "                lst = [line.strip() for line in lines[0::2]]\n",
    "                random_ua = random.choice(lst)\n",
    "                return random_ua  \n",
    "            \n",
    "        user_agent = get_random_ua()\n",
    "        headers = {'user-agent': user_agent}\n",
    "        response = requests.get(url, headers)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout occured')\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print('Too many redirects')\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSL error occured')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request exception')\n",
    "   \n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    def kickstart(self):\n",
    "        import time\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "            if self.sleep_interval > 0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 10 # how many webpages to scrapge\n",
    "\n",
    "\n",
    "def quotes_parser(content):\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    quotes = soup.find_all('article', {'class':'product_pod'})\n",
    "    quotes2 = [book.text.strip().split('\\n') for book in quotes]\n",
    "    quotes3 = [book[0] for book in quotes2]\n",
    "    quotes3    \n",
    "    return quotes3\n",
    "    \n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
